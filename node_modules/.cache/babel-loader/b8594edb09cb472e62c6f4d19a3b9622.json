{"ast":null,"code":"var _jsxFileName = \"/home/dolan/Downloads/DaveDotCom_/src/screens/data/Projects.js\",\n    _s = $RefreshSig$();\n\nimport React, { useState } from \"react\";\nimport ProjectData from './ProjectData';\nimport classes from '../css/Projects.module.css';\nimport github from './github.png';\nimport Button from 'react-bootstrap/Button';\nimport { alignPropType } from \"react-bootstrap/esm/types\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst Projects = () => {\n  _s();\n\n  var milk = new ProjectData('Milk', 'https://github.com/DavidHostler/Milk', 'Milk (it is a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that receives an image of a given size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.');\n  var gauss = new ProjectData('Gaussian', 'https://github.com/DavidHostler/Pytorch-Gaussian-NN', 'This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \"unlearn\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNNs, LSTMs or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function.');\n  var draco = new ProjectData('Draco', 'https://github.com/DavidHostler/Draco', 'This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I would implement my solution in C++ using my current understanding of the Python code at the time. In doing so, I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias) This project allowed me to play around with a super fast RL simulation and learn more about the direct behaviour of deep learning models from a mathematical point of view, as I had to hard-code the linear transformations and take the derivatives of activation functions numerically. If using this model, depending on the user architecture you might want to consider implementing a Big Float datatype to prevent Nan values as weights, biases or gradients.');\n  const projects = [gauss, milk, draco];\n  const [project, setProjects] = useState(\"\");\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"div\", {\n      children: projects.map(project => /*#__PURE__*/_jsxDEV(\"button\", {\n        style: {\n          color: \"#000000\",\n          backgroundColor: \"#d4af37\",\n          // borderRadius: '15px',\n          // border: 'none',\n          margin: \"1%\",\n          boxShadow: \"5px 5px 3px rgba(46, 46, 46, 0.62)\"\n        },\n        className: \"custom_subtitle\",\n        type: \"button\",\n        onClick: () => setProjects(project),\n        children: project.title\n      }, project, false, {\n        fileName: _jsxFileName,\n        lineNumber: 39,\n        columnNumber: 9\n      }, this))\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 36,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"center_all\",\n      children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n        className: classes['center'],\n        children: project.title\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 60,\n        columnNumber: 21\n      }, this), /*#__PURE__*/_jsxDEV(\"img\", {\n        src: github // 'https://static01.nyt.com/images/2016/09/28/us/28xp-pepefrog/28xp-pepefrog-superJumbo.jpg'\n        ,\n        style: {\n          borderRadius: '50%'\n        }\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 70,\n        columnNumber: 21\n      }, this), /*#__PURE__*/_jsxDEV(\"h5\", {\n        children: project.body\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 77,\n        columnNumber: 21\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 59,\n      columnNumber: 17\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 34,\n    columnNumber: 9\n  }, this);\n};\n\n_s(Projects, \"9bPMSjqfdGJNPVFqO5NSAZOJAeE=\");\n\n_c = Projects;\nexport default Projects;\n\nvar _c;\n\n$RefreshReg$(_c, \"Projects\");","map":{"version":3,"sources":["/home/dolan/Downloads/DaveDotCom_/src/screens/data/Projects.js"],"names":["React","useState","ProjectData","classes","github","Button","alignPropType","Projects","milk","gauss","draco","projects","project","setProjects","map","color","backgroundColor","margin","boxShadow","title","borderRadius","body"],"mappings":";;;AAAA,OAAOA,KAAP,IAAgBC,QAAhB,QAAgC,OAAhC;AACA,OAAOC,WAAP,MAAwB,eAAxB;AACA,OAAOC,OAAP,MAAoB,4BAApB;AACA,OAAOC,MAAP,MAAmB,cAAnB;AACA,OAAOC,MAAP,MAAmB,wBAAnB;AACA,SAASC,aAAT,QAA8B,2BAA9B;;;AAIA,MAAMC,QAAQ,GAAG,MAAM;AAAA;;AACnB,MAAIC,IAAI,GAAG,IAAIN,WAAJ,CACP,MADO,EAEP,sCAFO,EAGP,mRAHO,CAAX;AAMA,MAAIO,KAAK,GAAG,IAAIP,WAAJ,CACR,UADQ,EAER,qDAFQ,EAGR,+tCAHQ,CAAZ;AAMA,MAAIQ,KAAK,GAAG,IAAIR,WAAJ,CACR,OADQ,EAER,uCAFQ,EAGR,+mCAHQ,CAAZ;AAMA,QAAMS,QAAQ,GAAG,CAACF,KAAD,EAAQD,IAAR,EAAcE,KAAd,CAAjB;AACA,QAAM,CAACE,OAAD,EAAUC,WAAV,IAAyBZ,QAAQ,CAAC,EAAD,CAAvC;AAEA,sBAEI;AAAA,4BAEA;AAAA,gBAECU,QAAQ,CAACG,GAAT,CAAaF,OAAO,iBACrB;AAEA,QAAA,KAAK,EAAE;AAACG,UAAAA,KAAK,EAAC,SAAP;AACCC,UAAAA,eAAe,EAAC,SADjB;AAEC;AACA;AACPC,UAAAA,MAAM,EAAE,IAJF;AAIQC,UAAAA,SAAS,EAAE;AAJnB,SAFP;AAQA,QAAA,SAAS,EAAC,iBARV;AASA,QAAA,IAAI,EAAC,QATL;AAYA,QAAA,OAAO,EAAE,MAAIL,WAAW,CAACD,OAAD,CAZxB;AAAA,kBAaKA,OAAO,CAACO;AAbb,SAUKP,OAVL;AAAA;AAAA;AAAA;AAAA,cADC;AAFD;AAAA;AAAA;AAAA;AAAA,YAFA,eAyBQ;AAAK,MAAA,SAAS,EAAG,YAAjB;AAAA,8BACI;AAAI,QAAA,SAAS,EAAET,OAAO,CAAC,QAAD,CAAtB;AAAA,kBAAmCS,OAAO,CAACO;AAA3C;AAAA;AAAA;AAAA;AAAA,cADJ,eAWI;AAAK,QAAA,GAAG,EAAEf,MAAV,CAEA;AAFA;AAGA,QAAA,KAAK,EAAE;AAACgB,UAAAA,YAAY,EAAE;AAAf;AAHP;AAAA;AAAA;AAAA;AAAA,cAXJ,eAkBI;AAAA,kBAAKR,OAAO,CAACS;AAAb;AAAA;AAAA;AAAA;AAAA,cAlBJ;AAAA;AAAA;AAAA;AAAA;AAAA,YAzBR;AAAA;AAAA;AAAA;AAAA;AAAA,UAFJ;AAmDH,CAzED;;GAAMd,Q;;KAAAA,Q;AA6EN,eAAeA,QAAf","sourcesContent":["import React, { useState } from \"react\";\nimport ProjectData from './ProjectData';\nimport classes from '../css/Projects.module.css';\nimport github from './github.png';\nimport Button from 'react-bootstrap/Button';\nimport { alignPropType } from \"react-bootstrap/esm/types\";\n\n\n\nconst Projects = () => {\n    var milk = new ProjectData(\n        'Milk',\n        'https://github.com/DavidHostler/Milk',\n        'Milk (it is a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that receives an image of a given size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.'\n    );\n    \n    var gauss = new ProjectData(\n        'Gaussian',\n        'https://github.com/DavidHostler/Pytorch-Gaussian-NN',\n        'This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \"unlearn\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNNs, LSTMs or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function.'\n    );\n    \n    var draco = new ProjectData(\n        'Draco',\n        'https://github.com/DavidHostler/Draco',\n        'This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I would implement my solution in C++ using my current understanding of the Python code at the time. In doing so, I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias) This project allowed me to play around with a super fast RL simulation and learn more about the direct behaviour of deep learning models from a mathematical point of view, as I had to hard-code the linear transformations and take the derivatives of activation functions numerically. If using this model, depending on the user architecture you might want to consider implementing a Big Float datatype to prevent Nan values as weights, biases or gradients.'\n    );\n\n    const projects = [gauss, milk, draco];\n    const [project, setProjects] = useState(\"\");\n\n    return (\n\n        <div>\n\n        <div>\n            \n        {projects.map(project=>(\n        <button \n\n        style={{color:\"#000000\",\n                backgroundColor:\"#d4af37\",\n                // borderRadius: '15px',\n                // border: 'none',\n         margin: \"1%\", boxShadow: \"5px 5px 3px rgba(46, 46, 46, 0.62)\"}} \n        \n        className=\"custom_subtitle\"\n        type=\"button\"\n        key={project}\n        \n        onClick={()=>setProjects(project)}>\n            {project.title}\n        </button>\n        \n        ))}\n\n\n        </div>    \n                <div className = \"center_all\">\n                    <h1 className={classes['center']}>{project.title}</h1>\n\n                    {/* <h2 className={classes['center']}>\n                        <a href = {project.link}\n                        className={classes['hyperlink']}\n                        >\n                    \n                        {project.link}\n                        </a>\n                    </h2> */}\n                    <img src={github}\n                    \n                    // 'https://static01.nyt.com/images/2016/09/28/us/28xp-pepefrog/28xp-pepefrog-superJumbo.jpg'\n                    style={{borderRadius: '50%' \n                           }}\n                    />\n\n                    <h5>{project.body}</h5>\n                </div>\n\n        </div>\n    )\n\n}\n\n\n\nexport default Projects;"]},"metadata":{},"sourceType":"module"}