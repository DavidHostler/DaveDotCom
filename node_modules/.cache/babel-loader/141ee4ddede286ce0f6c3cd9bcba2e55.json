{"ast":null,"code":"var _jsxFileName = \"/home/dolan/Downloads/DaveDotCom_/src/screens/ProjectsScreen.js\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst ProjectsScreen = () => {\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: /*#__PURE__*/_jsxDEV(\"div\", {\n      children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n        className: \"custom-subTitle\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 7,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n        children: /*#__PURE__*/_jsxDEV(\"h1\", {\n          children: \"Welcome to Projects\"\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 9,\n          columnNumber: 9\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 8,\n        columnNumber: 5\n      }, this), /*#__PURE__*/_jsxDEV(\"h2\", {\n        children: \"Here is a list of some things that I've worked on!\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 11,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"h3\", {\n        children: \"My Github: https://github.com/DavidHostler\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 13,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"h2\", {\n        children: \"Machine Learning Related Projects\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 15,\n        columnNumber: 9\n      }, this), /*#__PURE__*/_jsxDEV(\"h3\", {\n        children: \"Here are some examples of ML projects that I've created in 2020, 2021: \\\"Milk\\\" (it's a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that  receives an image of a given size, and uses deep learning to \\\"hallucinate\\\" a more high resolution version of that image. https://github.com/DavidHostler/Milk This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \\\"unlearn\\\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNN's, LSTM's or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function. https://github.com/DavidHostler/Pytorch-Gaussian-NN DRACO https://github.com/DavidHostler/Draco This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I'd implement my solution in C++ using my current understanding of the Python code at the time. In doing so,  I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \\\"weights\\\"\\\", x is the input tensor, b is the bias)\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 16,\n        columnNumber: 9\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 6,\n      columnNumber: 5\n    }, this)\n  }, void 0, false, {\n    fileName: _jsxFileName,\n    lineNumber: 5,\n    columnNumber: 9\n  }, this);\n};\n\n_c = ProjectsScreen;\nexport default ProjectsScreen;\n\nvar _c;\n\n$RefreshReg$(_c, \"ProjectsScreen\");","map":{"version":3,"sources":["/home/dolan/Downloads/DaveDotCom_/src/screens/ProjectsScreen.js"],"names":["ProjectsScreen"],"mappings":";;;AAEA,MAAMA,cAAc,GAAG,MAAM;AACzB,sBACI;AAAA,2BACJ;AAAA,8BACI;AAAI,QAAA,SAAS,EAAC;AAAd;AAAA;AAAA;AAAA;AAAA,cADJ,eAEA;AAAA,+BACI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,cAFA,eAKI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cALJ,eAOI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAPJ,eASI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cATJ,eAUI;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,cAVJ;AAAA;AAAA;AAAA;AAAA;AAAA;AADI;AAAA;AAAA;AAAA;AAAA,UADJ;AAkEH,CAnED;;KAAMA,c;AAqEN,eAAeA,cAAf","sourcesContent":["\n\nconst ProjectsScreen = () => {\n    return(\n        <div >\n    <div>\n        <h1 className=\"custom-subTitle\"></h1>\n    <div>\n        <h1>Welcome to Projects</h1>\n    </div>\n        <h2>Here is a list of some things that I've worked on!</h2>\n        \n        <h3>My Github: https://github.com/DavidHostler</h3>\n\n        <h2>Machine Learning Related Projects</h2>\n        <h3>Here are some examples of ML projects that I've created \n            in 2020, 2021:\n\n            \"Milk\" (it's a weird name I know, but some people like a nice glass of Milk now and then).\n            This is a super resolution generative adversarial network that  receives an image of a given\n            size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.            \n            \n\n            https://github.com/DavidHostler/Milk\n\n\n            This is a little research project I worked on briefly in late 2020 and early 2021.\n            I had volunteered to help some grad students in the University of Toronto Scarborough \n            materials science research project with implementing a single hidden-layer neural network\n            in order to model high-precision Gaussian functions given a single input value.\n\n            The idea is that if a given element or compound has a known excitation energy known to a \n            certain accuracy, then we could improve this accuracy by using a neural network to approximate\n            the Gaussian distribution function centred around this excitation energy eigenvalue.\n\n            The results were that the required neural architecture was insufficient for such a desired result,\n            since training the neural network on successive eigenvalue training data had the effect of causing the \n            model to \"unlearn\" anything it knew of previous eigenvalue distributions.\n            Therefore, I posit that the only way to implement this method successfully would be to ascribe to the\n            model an attention window- e.g. RNN's, LSTM's or preferably Transformers.\n\n            These models were deemed too complicated for the desired architecture and so they decided to implement\n            the energy distribution functions directly by hard-coding the distribution function.\n\n            https://github.com/DavidHostler/Pytorch-Gaussian-NN\n\n\n\n            DRACO\n            https://github.com/DavidHostler/Draco\n\n\n            This one is a favorite for a couple of reasons. I was attempting to implement \n            a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients\n            or DDPG for short. I wanted to maximize the speed of my agent as well.\n            \n            Python is known to run about ~ 20 times more slowly than C/C++, \n            so I figured that I'd implement my solution in C++ using my current understanding \n            of the Python code at the time.\n\n            In doing so,  I ran into a few build issues with the Tensorflow C API and figured that\n            since DDPG only uses deep hidden layers (i.e. linear transformations of the form \n            Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias)\n\n        </h3>\n    </div>\n    </div>\n\n    )\n}\n\nexport default ProjectsScreen;"]},"metadata":{},"sourceType":"module"}