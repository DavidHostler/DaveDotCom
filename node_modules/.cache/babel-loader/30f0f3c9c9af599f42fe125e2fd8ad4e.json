{"ast":null,"code":"var _jsxFileName = \"/home/dolan/Downloads/DaveDotCom_/src/screens/data/Projects.js\",\n    _s = $RefreshSig$();\n\nimport React, { useState } from \"react\";\nimport ProjectData from './ProjectData'; //Define the different projects created\n\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nvar milk = new ProjectData('Milk', 'https://github.com/DavidHostler/Milk', 'Milk (it is a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that receives an image of a given size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.');\nvar gauss = new ProjectData('Gaussian', 'https://github.com/DavidHostler/Pytorch-Gaussian-NN', 'This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \"unlearn\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNNs, LSTMs or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function.');\nvar draco = new ProjectData('Draco', 'https://github.com/DavidHostler/Draco', 'This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I would implement my solution in C++ using my current understanding of the Python code at the time. In doing so, I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias) This project allowed me to play around with a super fast RL simulation and learn more about the direct behaviour of deep learning models from a mathematical point of view, as I had to hard-code the linear transformations and take the derivatives of activation functions numerically. If using this model, depending on the user architecture you might want to consider implementing a Big Float datatype to prevent Nan values as weights, biases or gradients.');\n\nconst Projects = () => {\n  _s();\n\n  const projects = [\"gauss\", \"draco\", \"milk\"];\n  const [project, setProjects] = useState(\"\");\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"div\", {\n      children: projects.map(project => /*#__PURE__*/_jsxDEV(\"button\", {\n        type: \"button\",\n        onClick: () => setProjects(project),\n        children: project.toLocaleLowerCase()\n      }, project, false, {\n        fileName: _jsxFileName,\n        lineNumber: 36,\n        columnNumber: 9\n      }, this))\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 33,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"center_all\",\n      children: /*#__PURE__*/_jsxDEV(\"p\", {\n        children: project\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 49,\n        columnNumber: 21\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 48,\n      columnNumber: 17\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 31,\n    columnNumber: 9\n  }, this);\n};\n\n_s(Projects, \"9bPMSjqfdGJNPVFqO5NSAZOJAeE=\");\n\n_c = Projects;\nexport default Projects;\n\nvar _c;\n\n$RefreshReg$(_c, \"Projects\");","map":{"version":3,"sources":["/home/dolan/Downloads/DaveDotCom_/src/screens/data/Projects.js"],"names":["React","useState","ProjectData","milk","gauss","draco","Projects","projects","project","setProjects","map","toLocaleLowerCase"],"mappings":";;;AAAA,OAAOA,KAAP,IAAgBC,QAAhB,QAAgC,OAAhC;AACA,OAAOC,WAAP,MAAwB,eAAxB,C,CAEA;;;AACA,IAAIC,IAAI,GAAG,IAAID,WAAJ,CACP,MADO,EAEP,sCAFO,EAGP,mRAHO,CAAX;AAMA,IAAIE,KAAK,GAAG,IAAIF,WAAJ,CACR,UADQ,EAER,qDAFQ,EAGR,+tCAHQ,CAAZ;AAMA,IAAIG,KAAK,GAAG,IAAIH,WAAJ,CACR,OADQ,EAER,uCAFQ,EAGR,+mCAHQ,CAAZ;;AAQA,MAAMI,QAAQ,GAAG,MAAM;AAAA;;AACnB,QAAMC,QAAQ,GAAG,CAAC,OAAD,EAAU,OAAV,EAAmB,MAAnB,CAAjB;AACA,QAAM,CAACC,OAAD,EAAUC,WAAV,IAAyBR,QAAQ,CAAC,EAAD,CAAvC;AAEA,sBAEI;AAAA,4BAEA;AAAA,gBAECM,QAAQ,CAACG,GAAT,CAAaF,OAAO,iBACrB;AACA,QAAA,IAAI,EAAC,QADL;AAIA,QAAA,OAAO,EAAE,MAAIC,WAAW,CAACD,OAAD,CAJxB;AAAA,kBAKKA,OAAO,CAACG,iBAAR;AALL,SAEKH,OAFL;AAAA;AAAA;AAAA;AAAA,cADC;AAFD;AAAA;AAAA;AAAA;AAAA,YAFA,eAiBQ;AAAK,MAAA,SAAS,EAAG,YAAjB;AAAA,6BACI;AAAA,kBAAIA;AAAJ;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,YAjBR;AAAA;AAAA;AAAA;AAAA;AAAA,UAFJ;AA0BH,CA9BD;;GAAMF,Q;;KAAAA,Q;AAkCN,eAAeA,QAAf","sourcesContent":["import React, { useState } from \"react\";\nimport ProjectData from './ProjectData';\n\n//Define the different projects created\nvar milk = new ProjectData(\n    'Milk',\n    'https://github.com/DavidHostler/Milk',\n    'Milk (it is a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that receives an image of a given size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.'\n);\n\nvar gauss = new ProjectData(\n    'Gaussian',\n    'https://github.com/DavidHostler/Pytorch-Gaussian-NN',\n    'This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \"unlearn\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNNs, LSTMs or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function.'\n);\n\nvar draco = new ProjectData(\n    'Draco',\n    'https://github.com/DavidHostler/Draco',\n    'This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I would implement my solution in C++ using my current understanding of the Python code at the time. In doing so, I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias) This project allowed me to play around with a super fast RL simulation and learn more about the direct behaviour of deep learning models from a mathematical point of view, as I had to hard-code the linear transformations and take the derivatives of activation functions numerically. If using this model, depending on the user architecture you might want to consider implementing a Big Float datatype to prevent Nan values as weights, biases or gradients.'\n);\n\n\n\nconst Projects = () => {\n    const projects = [\"gauss\", \"draco\", \"milk\"];\n    const [project, setProjects] = useState(\"\");\n\n    return (\n\n        <div>\n\n        <div>\n            \n        {projects.map(project=>(\n        <button \n        type=\"button\"\n        key={project}\n        \n        onClick={()=>setProjects(project)}>\n            {project.toLocaleLowerCase()}\n        </button>\n        \n        ))}\n\n\n        </div>    \n                <div className = \"center_all\">\n                    <p>{project}</p>\n                </div>\n\n        </div>\n    )\n\n}\n\n\n\nexport default Projects;"]},"metadata":{},"sourceType":"module"}