{"ast":null,"code":"var _jsxFileName = \"/home/dolan/Downloads/DaveDotCom_/src/screens/data/Projects.js\",\n    _s = $RefreshSig$();\n\nimport React, { useState } from \"react\";\nimport ProjectData from './ProjectData';\nimport classes from '../css/Projects.module.css';\nimport Button from 'react-bootstrap/Button';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\n\nconst Projects = () => {\n  _s();\n\n  var milk = new ProjectData('Milk', 'https://github.com/DavidHostler/Milk', 'Milk (it is a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that receives an image of a given size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.');\n  var gauss = new ProjectData('Gaussian', 'https://github.com/DavidHostler/Pytorch-Gaussian-NN', 'This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \"unlearn\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNNs, LSTMs or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function.');\n  var draco = new ProjectData('Draco', 'https://github.com/DavidHostler/Draco', 'This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I would implement my solution in C++ using my current understanding of the Python code at the time. In doing so, I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias) This project allowed me to play around with a super fast RL simulation and learn more about the direct behaviour of deep learning models from a mathematical point of view, as I had to hard-code the linear transformations and take the derivatives of activation functions numerically. If using this model, depending on the user architecture you might want to consider implementing a Big Float datatype to prevent Nan values as weights, biases or gradients.');\n  const projects = [gauss, milk, draco];\n  const [project, setProjects] = useState(\"\");\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"div\", {\n      children: projects.map(project => /*#__PURE__*/_jsxDEV(\"button\", {\n        style: {\n          color: \"#000000\",\n          backgroundColor: \"#d4af37\",\n          margin: \"1%\",\n          boxShadow: \"5px 5px 3px rgba(46, 46, 46, 0.62)\"\n        },\n        className: \"btn btn-primary\",\n        type: \"button\",\n        onClick: () => setProjects(project),\n        children: project.title\n      }, project, false, {\n        fileName: _jsxFileName,\n        lineNumber: 38,\n        columnNumber: 9\n      }, this))\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 35,\n      columnNumber: 9\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      className: \"center_all\",\n      children: [/*#__PURE__*/_jsxDEV(\"h1\", {\n        className: classes['center'],\n        children: project.title\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 55,\n        columnNumber: 21\n      }, this), /*#__PURE__*/_jsxDEV(\"h2\", {\n        className: classes['center'],\n        children: /*#__PURE__*/_jsxDEV(\"a\", {\n          href: project.link,\n          className: classes['hyperlink'],\n          children: project.link\n        }, void 0, false, {\n          fileName: _jsxFileName,\n          lineNumber: 58,\n          columnNumber: 25\n        }, this)\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 57,\n        columnNumber: 21\n      }, this), /*#__PURE__*/_jsxDEV(\"h5\", {\n        children: project.body\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 65,\n        columnNumber: 21\n      }, this)]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 54,\n      columnNumber: 17\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 33,\n    columnNumber: 9\n  }, this);\n};\n\n_s(Projects, \"9bPMSjqfdGJNPVFqO5NSAZOJAeE=\");\n\n_c = Projects;\nexport default Projects;\n\nvar _c;\n\n$RefreshReg$(_c, \"Projects\");","map":{"version":3,"sources":["/home/dolan/Downloads/DaveDotCom_/src/screens/data/Projects.js"],"names":["React","useState","ProjectData","classes","Button","Projects","milk","gauss","draco","projects","project","setProjects","map","color","backgroundColor","margin","boxShadow","title","link","body"],"mappings":";;;AAAA,OAAOA,KAAP,IAAgBC,QAAhB,QAAgC,OAAhC;AACA,OAAOC,WAAP,MAAwB,eAAxB;AACA,OAAOC,OAAP,MAAoB,4BAApB;AAEA,OAAOC,MAAP,MAAmB,wBAAnB;;;AAIA,MAAMC,QAAQ,GAAG,MAAM;AAAA;;AACnB,MAAIC,IAAI,GAAG,IAAIJ,WAAJ,CACP,MADO,EAEP,sCAFO,EAGP,mRAHO,CAAX;AAMA,MAAIK,KAAK,GAAG,IAAIL,WAAJ,CACR,UADQ,EAER,qDAFQ,EAGR,+tCAHQ,CAAZ;AAMA,MAAIM,KAAK,GAAG,IAAIN,WAAJ,CACR,OADQ,EAER,uCAFQ,EAGR,+mCAHQ,CAAZ;AAMA,QAAMO,QAAQ,GAAG,CAACF,KAAD,EAAQD,IAAR,EAAcE,KAAd,CAAjB;AACA,QAAM,CAACE,OAAD,EAAUC,WAAV,IAAyBV,QAAQ,CAAC,EAAD,CAAvC;AAEA,sBAEI;AAAA,4BAEA;AAAA,gBAECQ,QAAQ,CAACG,GAAT,CAAaF,OAAO,iBACrB;AACA,QAAA,KAAK,EAAE;AAACG,UAAAA,KAAK,EAAC,SAAP;AACCC,UAAAA,eAAe,EAAC,SADjB;AAENC,UAAAA,MAAM,EAAE,IAFF;AAEQC,UAAAA,SAAS,EAAE;AAFnB,SADP;AAIA,QAAA,SAAS,EAAG,iBAJZ;AAKA,QAAA,IAAI,EAAC,QALL;AAQA,QAAA,OAAO,EAAE,MAAIL,WAAW,CAACD,OAAD,CARxB;AAAA,kBASKA,OAAO,CAACO;AATb,SAMKP,OANL;AAAA;AAAA;AAAA;AAAA,cADC;AAFD;AAAA;AAAA;AAAA;AAAA,YAFA,eAqBQ;AAAK,MAAA,SAAS,EAAG,YAAjB;AAAA,8BACI;AAAI,QAAA,SAAS,EAAEP,OAAO,CAAC,QAAD,CAAtB;AAAA,kBAAmCO,OAAO,CAACO;AAA3C;AAAA;AAAA;AAAA;AAAA,cADJ,eAGI;AAAI,QAAA,SAAS,EAAEd,OAAO,CAAC,QAAD,CAAtB;AAAA,+BACI;AAAG,UAAA,IAAI,EAAIO,OAAO,CAACQ,IAAnB;AACA,UAAA,SAAS,EAAEf,OAAO,CAAC,WAAD,CADlB;AAAA,oBAGCO,OAAO,CAACQ;AAHT;AAAA;AAAA;AAAA;AAAA;AADJ;AAAA;AAAA;AAAA;AAAA,cAHJ,eAWI;AAAA,kBAAKR,OAAO,CAACS;AAAb;AAAA;AAAA;AAAA;AAAA,cAXJ;AAAA;AAAA;AAAA;AAAA;AAAA,YArBR;AAAA;AAAA;AAAA;AAAA;AAAA,UAFJ;AAwCH,CA9DD;;GAAMd,Q;;KAAAA,Q;AAkEN,eAAeA,QAAf","sourcesContent":["import React, { useState } from \"react\";\nimport ProjectData from './ProjectData';\nimport classes from '../css/Projects.module.css';\n\nimport Button from 'react-bootstrap/Button';\n\n\n\nconst Projects = () => {\n    var milk = new ProjectData(\n        'Milk',\n        'https://github.com/DavidHostler/Milk',\n        'Milk (it is a weird name I know, but some people like a nice glass of Milk now and then). This is a super resolution generative adversarial network that receives an image of a given size, and uses deep learning to \"hallucinate\" a more high resolution version of that image.'\n    );\n    \n    var gauss = new ProjectData(\n        'Gaussian',\n        'https://github.com/DavidHostler/Pytorch-Gaussian-NN',\n        'This is a little research project I worked on briefly in late 2020 and early 2021. I had volunteered to help some grad students in the University of Toronto Scarborough materials science research project with implementing a single hidden-layer neural network in order to model high-precision Gaussian functions given a single input value. The idea is that if a given element or compound has a known excitation energy known to a certain accuracy, then we could improve this accuracy by using a neural network to approximate the Gaussian distribution function centred around this excitation energy eigenvalue. The results were that the required neural architecture was insufficient for such a desired result, since training the neural network on successive eigenvalue training data had the effect of causing the model to \"unlearn\" anything it knew of previous eigenvalue distributions. Therefore, I posit that the only way to implement this method successfully would be to ascribe to the model an attention window- e.g. RNNs, LSTMs or preferably Transformers. These models were deemed too complicated for the desired architecture and so they decided to implement the energy distribution functions directly by hard-coding the distribution function.'\n    );\n    \n    var draco = new ProjectData(\n        'Draco',\n        'https://github.com/DavidHostler/Draco',\n        'This one is a favorite for a couple of reasons. I was attempting to implement a Reinforcement Learning agent using a technique called Deep Deterministic Policy Gradients or DDPG for short. I wanted to maximize the speed of my agent as well. Python is known to run about ~ 20 times more slowly than C/C++, so I figured that I would implement my solution in C++ using my current understanding of the Python code at the time. In doing so, I ran into a few build issues with the Tensorflow C API and figured that since DDPG only uses deep hidden layers (i.e. linear transformations of the form Ax + b = y, where A is a matrix a.k.a. \"weights\"\", x is the input tensor, b is the bias) This project allowed me to play around with a super fast RL simulation and learn more about the direct behaviour of deep learning models from a mathematical point of view, as I had to hard-code the linear transformations and take the derivatives of activation functions numerically. If using this model, depending on the user architecture you might want to consider implementing a Big Float datatype to prevent Nan values as weights, biases or gradients.'\n    );\n\n    const projects = [gauss, milk, draco];\n    const [project, setProjects] = useState(\"\");\n\n    return (\n\n        <div>\n\n        <div>\n            \n        {projects.map(project=>(\n        <button \n        style={{color:\"#000000\",\n                backgroundColor:\"#d4af37\",\n         margin: \"1%\", boxShadow: \"5px 5px 3px rgba(46, 46, 46, 0.62)\"}} \n        className = \"btn btn-primary\"\n        type=\"button\"\n        key={project}\n        \n        onClick={()=>setProjects(project)}>\n            {project.title}\n        </button>\n        \n        ))}\n\n\n        </div>    \n                <div className = \"center_all\">\n                    <h1 className={classes['center']}>{project.title}</h1>\n\n                    <h2 className={classes['center']}>\n                        <a href = {project.link}\n                        className={classes['hyperlink']}\n                        >\n                        {project.link}\n                        </a>\n                    </h2>\n\n                    <h5>{project.body}</h5>\n                </div>\n\n        </div>\n    )\n\n}\n\n\n\nexport default Projects;"]},"metadata":{},"sourceType":"module"}